8. In this problem we will implement a regularized least squares classifier for the MNIST data set. The task
is to classify handwritten images of numbers between $0$ to $9$.\\

You are \textbf{NOT} allowed to use
any of the prebuilt  classifiers in \verb|sklearn|.  Feel free to use any method from \verb|numpy|
or \verb|scipy|. Remember: if you are inverting a matrix in your code, you are probably doing something wrong (Hint: look at \verb|scipy.linalg.solve|).\\

Get the data from \url{https://pypi.python.org/pypi/python-mnist}. \\
Load the data as follows:
\begin{verbatim}
from mnist import MNIST

def load_dataset():
    mndata = MNIST('./data/')
    X_train, labels_train = map(np.array, mndata.load_training())
    X_test, labels_test = map(np.array, mndata.load_testing())
    X_train = X_train/255.0
    X_test = X_test/255.0
\end{verbatim}

Each example has features $x_i \in \R^d$ (with $d=28*28=784$) and label $z_j \in \{0,\dots,9\}$.  
You can visualize a single example $x_i$ with \texttt{imshow} after reshaping it to its original $28 \times 28$ image shape (and noting that the label $z_j$ is accurate).
We wish to learn a predictor $\widehat{f}$ that takes as input a vector in $\R^d$ and outputs an index in $\{0,\dots,9\}$.
We define our training and testing classification error on a predictor $f$ as
\begin{align*}
\widehat{\epsilon}_{\textrm{train}}(f) &=
\frac{1}{N _{\textrm{train}}} \sum_{(x,z)\in \textrm{Training Set}}     \1\{ f(x) \neq z \}
\\
  \widehat{\epsilon}_{\textrm{test}}(f) &=
  \frac{1}{N _{\textrm{test}}} \sum_{(x,z)\in \textrm{Test Set}}     \1\{ f(x) \neq z \} 
\end{align*}

We will use one-hot encoding of the labels, i.e. of $(x,z)$ the original label $z \in \{0, \ldots, 9\}$ is mapped to the standard basis vector $e_z$ where $e_z$ is a vector of all zeros except for a $1$ in the $z$th position.
We adopt the notation where we have $n$ data points in our training objective with features $x_i \in \R^d$ and label one-hot encoded as $y_i \in \{0,1\}^k$ where in this case $k=10$ since there are 10 digits.

\begin{enumerate}
\item \points{10} In this problem we will choose a linear classifier to minimize the regularized least squares objective:
\begin{align*}\widehat{W} = \text{argmin}_{W \in \R^{d \times k}} \sum_{i=0}^{n} \| W^Tx_{i} - y_{i} \|^{2}_{2} + \lambda \|W\|_{F}^{2}
\end{align*}
 Note that $\|W\|_{F}$ corresponds to the Frobenius norm of $W$, i.e. $\|W\|_{F}^{2} = \sum_{i=1}^d \sum_{j=1}^k W_{i,j}^2$.
 To classify a point $x_i$ we will use the rule $\arg\max_{j=0,\dots,9} e_j^T \widehat{W}^T x_i$.
Note that if $W = \begin{bmatrix} w_1 & \dots & w_k \end{bmatrix}$ then
\begin{align*}
\sum_{i=0}^{n} \| W^Tx_{i} - y_{i} \|^{2}_{2} + \lambda \|W\|_{F}^{2} &= \sum_{j=0}^k \left[  \sum_{i=1}^n ( e_j^T W^T x_i - e_j^T y_i)^2 + \lambda \| W e_j \|^2 \right] \\
&= \sum_{j=0}^k \left[  \sum_{i=1}^n ( w_j^T x_i - e_j^T y_i)^2 + \lambda \| w_j \|^2 \right] \\
&= \sum_{j=0}^k \left[  \| X w_j - Y e_j\|^2 + \lambda \| w_j \|^2 \right]
\end{align*}
where $X = \begin{bmatrix} x_1 & \dots & x_n \end{bmatrix}^\top \in \R^{n \times d}$ and $Y = \begin{bmatrix} y_1 & \dots & y_n \end{bmatrix}^\top \in \R^{n \times k}$.
Show that
\begin{align*}
\widehat{W} = (X^T X + \lambda I)^{-1} X^T Y
\end{align*} 

%% ANSWER a.
\input{body/a8_a.tex}

\item \points{10} 
\begin{itemize}
    \item Code up a function \verb|train| that takes as input $X \in\R^{n \times d}$, $Y \in \{0,1\}^{n \times k}$, $\lambda > 0$ and returns $\widehat{W}$.
    \item Code up a function  \verb|predict| that takes as input $W \in \R^{d \times k}$, $X' \in\R^{m \times d}$ and returns an $m$-length vector with the $i$th entry equal to $\arg\max_{j=0,\dots,9} e_j^T W^T x_i'$ where $x_i'$ is a column vector representing the $i$th example from $X'$.
    \item Train $\widehat{W}$ on the MNIST training data with $\lambda = 10^{-4}$ and make label predictions on the test data. 
{\bf What is the training and testing error?} Note that they should both be about $15\%$. 
\end{itemize}

\end{enumerate}
