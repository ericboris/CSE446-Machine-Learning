\begin{quote}
    In general {\bf it is not true} that $\E_{\text{train}}[ \widehat{\epsilon}_{\text{train}} ( \widehat{f} )] = \E_{\text{train}} [ \epsilon (\widehat{f}) ]$. This is because, above, to go from $\frac{1}{N_{\textrm{test}}} \, \E_{\textrm{test}} \left[  \sum_{(x, \, y)\in S_{\textrm{test}}} \left( \widehat{f}(x) - y \right)^2 \right]$ to $\frac{1}{N_{\textrm{test}}} \, N_{\textrm{test}} \, \E_{(x,\, y) \sim \mathcal{D}} \left[  \left( \widehat{f}(x) - y \right)^2 \right]$ we had to know that $x_i$, $y_i$, and $\widehat{f}$ were iid from $S_{\textrm{train}}$. We knew this to be the case above because we're told that $x_i$ and $y_i$ are iid and that $\widehat{f}$ was found using only training data. However, since we don't always know how $\widehat{f}$ was found, we can't always assume that $\widehat{f}$ is iid from $S_{\textrm{train}}$, and thus it's not generally true that $\E_{\text{train}}[ \widehat{\epsilon}_{\text{train}} ( \widehat{f} )] = \E_{\text{train}} [ \epsilon (\widehat{f}) ]$.
\end{quote}

