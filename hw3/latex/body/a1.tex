{\bf Problem 1}
\begin{enumerate}
    \item {\bf False}: SVM only maximizes the margin, it doesn't minimize generalization error among linear classifiers.
    \item {\bf Decrease}: Lower values of $\sigma$ allow the model to be more expressive.
    \item {\bf True}: Neural networks often have non-convex loss functions with only local minima.
    \item {\bf False}: This can lead to problems. It's better to initialize weights to random values. 
    \item {\bf True}: Without nonlinearities in the network, the network can only learn linear functions.
    \item {\bf True}: We mitigate this by using SGD to improve the runtime of the backward pass.
\end{enumerate}
